# Тестовое задание: Python‑разработчик (5G NR/V2X)

## Цель

Нужно построить небольшой ETL‑конвейер для логов 5G NR/V2X. Конвейер должен принимать различные форматы логов, нормализовать записи в единый формат, рассчитывать основные сетевые метрики и генерировать отчёты/графики. Проект должен легко расширяться под новые форматы (симуляторы Sionna, ns‑3, OMNeT++, SUMO) и новые метрики.

## Задачи

### 1. Парсер логов (стриминг)

- Входные форматы: CSV и NDJSON. В дальнейшем будут добавляться ns‑3 CSV и другие форматы, поэтому архитектуру парсера нужно спроектировать расширяемой.
- Парсер должен читать входные файлы стримингово, без загрузки всего файла в память. Можно реализовать генератор, который возвращает записи по одной.
- Выводить данные в унифицированной структуре (например, Pydantic‑модель или `dict`) со следующими полями:
  - `ts_us:int` – временная метка в микросекундах;
  - `event:str` – тип события (`"tx"` или `"rx"`);
  - `src:str` – отправитель;
  - `dst:str` – получатель;
  - `pkt_id:str` – идентификатор пакета;
  - `app:str` – тип приложения (например, `"BSM"`);
  - `bytes:int` – размер полезной нагрузки;
  - `rssi_dbm:float|None` – мощность сигнала (опционально);
  - `sinr_db:float|None` – SINR (опционально);
  - `drop_reason:str|None` – причина дропа (при наличии).
- Пример логики: если файл содержит лишние поля, игнорировать их; если требуемого поля нет, подставлять `None`. Пропускать строки с некорректным форматом, логируя ошибки.

### 2. Подсчёт метрик

Для каждой пары `src→dst`, по каждому типу `app` и в целом по файлу нужно вычислить:

- **PDR** (Packet Delivery Ratio) = `rx_count / tx_count`.
- **Latency** (задержка) для каждой пары пакетов TX/RX с одинаковым `pkt_id`: считать разность между временем RX и временем TX, потом агрегировать:
  - среднее значение;
  - медиану (p50);
  - 95‑й процентиль (p95);
  - стандартное отклонение.
- Метрики нужно агрегировать по фиксированным временным окнам (например, 1 с). В дальнейшем параметр окна должен настраиваться.
- Нужно корректно обрабатывать аномалии:
  - дубликаты TX (одинаковый `pkt_id` отправляется несколько раз);
  - повторные RX для одного `pkt_id`;
  - RX без соответствующего TX (сторонний пакет);
  - несоответствие направления (пакет пришёл не тому получателю);
  - отрицательная задержка (временные метки неупорядочены);
  - отсутствие SINR/RSSI.
- Все аномалии нужно логировать и включать в отдельный отчёт.
- Результаты метрик следует сохранять в CSV и Parquet для каждого разреза (всего, по парам, по приложениям, по окнам) и предоставлять краткую сводку (число обработанных пакетов, PDR, статистика задержек и т. д.).

### 3. CLI‑утилита

Напишите командную утилиту `nr‑metrics` с помощью `click` или аналогичного фреймворка. Она должна поддерживать следующие подкоманды:

1. `parse` — принимает один или несколько входных логов (`.csv` или `.ndjson`), определяет нужный парсер, транслирует записи в унифицированный JSON/NDJSON и сохраняет результат.
2. `metrics` — читает унифицированный поток, рассчитывает метрики, сохраняет таблицы (CSV и Parquet) и выводит краткую статистику на экран.
3. `plot` — строит графики по полученным Parquet‑файлам:
   - PDR по временным окнам для выбранной пары `src→dst`;
   - CDF задержек (график распределения);
   - средняя задержка vs. средний SINR (или RSSI).
   Графики нужно сохранять в указанную папку (`.png`).

CLI должна предоставлять `--help` для каждой команды, поддерживать настройку окна агрегации и формата вывода, а также использовать логирование для вывода прогресса.

### 4. Качество кода

- Код должен быть совместим с Python ≥ 3.10 и использовать аннотации типов.
- Следует придерживаться PEP 8 (используйте `ruff`, `flake8` или `black`), а также проверять типы (`mypy` приветствуется).
- Напишите несколько юнит‑тестов (например, с `pytest`), которые подтверждают корректность расчёта PDR и задержек на эталонном датасете (см. ниже). Тесты должны запускаться быстро и проверять крайние случаи (дубли, пропуски, отрицательная задержка).
- README или docs должны описывать, как запускать ваш скрипт, пример вызова на тестовых данных, где искать результаты и графики.
- Бонус: предоставьте `Dockerfile` и `Makefile`/`poetry` для запуска всего окружения и демонстрации.

### 5. Дополнительные задания (по желанию)

Эти пункты не обязательны, но существенно улучшают оценку:

- **VoI‑фильтрация (Value‑of‑Information):** добавьте модуль, который присваивает каждому событию оценку `score = α·novelty + β·freshness + γ·hazard_flag` и отбрасывает события с низкой ценностью.
- **Адаптеры‑источники:** спроектируйте абстрактный класс `Parser` и реализуйте адаптеры `NdjsonParser`, `CsvParser` и заглушку `Ns3CsvParser` для будущего парсера логов ns‑3. Добавьте возможность динамически загружать новые парсеры.
- **Параллельная обработка:** реализуйте многопроцессорную обработку крупных файлов с прогресс‑баром (`tqdm`), чтобы ускорить обработку.
- **Docker:** подготовьте контейнер, который устанавливает зависимости и запускает `nr‑metrics` на тестовых данных командой `make demo`.

## Эталонный мини‑датасет

Для проверки правильности алгоритмов приложен небольшой NDJSON‑файл `sample.ndjson`. Он содержит два потока пакетов в обоих направлениях между автомобилями `car_12` и `car_33` (один поток `car_12→car_33` и обратный `car_33→car_12`). Структура файла:

```
{"ts_us":0,    "event":"tx","src":"car_12","dst":"car_33","pkt_id":"a1","app":"BSM","bytes":250,"rssi_dbm":-65,"sinr_db":18.2}
{"ts_us":200,  "event":"rx","src":"car_33","dst":"car_12","pkt_id":"a1","app":"BSM","bytes":250,"rssi_dbm":-67,"sinr_db":17.5}

{"ts_us":1000, "event":"tx","src":"car_12","dst":"car_33","pkt_id":"a2","app":"BSM","bytes":250}
{"ts_us":2000, "event":"tx","src":"car_12","dst":"car_33","pkt_id":"a3","app":"BSM","bytes":250}
{"ts_us":2150, "event":"rx","src":"car_33","dst":"car_12","pkt_id":"a3","app":"BSM","bytes":250}

{"ts_us":3000, "event":"tx","src":"car_12","dst":"car_33","pkt_id":"a4","app":"BSM","bytes":250}
{"ts_us":3250, "event":"rx","src":"car_33","dst":"car_12","pkt_id":"a4","app":"BSM","bytes":250}

{"ts_us":4000, "event":"tx","src":"car_12","dst":"car_33","pkt_id":"a5","app":"BSM","bytes":250}

{"ts_us":5000, "event":"tx","src":"car_12","dst":"car_33","pkt_id":"a6","app":"BSM","bytes":250}
{"ts_us":5100, "event":"rx","src":"car_33","dst":"car_12","pkt_id":"a6","app":"BSM","bytes":250}

// Второй поток в обратную сторону
{"ts_us":50,   "event":"tx","src":"car_33","dst":"car_12","pkt_id":"b1","app":"BSM","bytes":250}
{"ts_us":90,   "event":"rx","src":"car_12","dst":"car_33","pkt_id":"b1","app":"BSM","bytes":250}

{"ts_us":1050,"event":"tx","src":"car_33","dst":"car_12","pkt_id":"b2","app":"BSM","bytes":250}

{"ts_us":2050,"event":"tx","src":"car_33","dst":"car_12","pkt_id":"b3","app":"BSM","bytes":250}
{"ts_us":2250,"event":"rx","src":"car_12","dst":"car_33","pkt_id":"b3","app":"BSM","bytes":250}
```

_Примечание_: комментарии вроде `// Второй поток...` не входят в формат NDJSON, поэтому для реального парсинга удалите их.

### Проверочные ожидания

На основе этого датасета метрики должны получиться такими (разрешается небольшое отклонение по абсолютной величине, порядка 1e‑6):

- Для потока `car_12→car_33`:
  - TX = 6, RX = 4 → **PDR = 4/6 ≈ 0,6667**;
  - Задержки (мкс): `a1=200`, `a3=150`, `a4=250`, `a6=100`;
    - mean ≈ 175;
    - p50 ≈ 175;
    - p95 ≈ 250;
    - std ≈ 62,9.

- Для потока `car_33→car_12`:
  - TX = 3, RX = 2 → **PDR = 2/3 ≈ 0,6667**;
  - Задержки (мкс): `b1=40`, `b3=200`;
    - mean = 120;
    - p50 = 120;
    - p95 = 200;
    - std ≈ 113,1.

- Общая PDR (для всех направлений) должна совпадать с отношением суммарных RX к TX.
- Окна по 1 с совпадают с общими значениями из‑за малых временных диапазонов; тем не менее, ваша реализация должна корректно агрегировать по окнам.

Рекомендуется написать хотя бы один `pytest`‑тест, который читает этот файл, запускает ваш расчёт и сверяет полученные метрики с ожидаемыми.

## Результат

При сдаче тестового задания нужно предоставить ссылку на публичный репозиторий (GitHub или аналог), который содержит:

1. **Исходный код** парсеров, процессора метрик, экспорта и визуализации;
2. **CLI‑утилиту** `nr‑metrics` (настроенную как entrypoint);
3. **Тесты** и данные (`sample.ndjson`); 
4. **Документацию/README** с инструкцией по запуску (например, команды: `nr‑metrics parse sample.ndjson`, `nr‑metrics metrics …`, `nr‑metrics plot …`), описанием структуры результатов и графиков;
5. При наличии — **Dockerfile** и скрипт `make demo`, который запускает демонстрацию на тестовых данных и сохраняет артефакты (CSV/Parquet и картинки) в `./artifacts/`.

В README укажите, сколько времени потратили на выполнение задания, что удалось сделать, а что оставили на потом. 

## Оценка

Работа будет оцениваться по следующим критериям (суммарно 100 баллов):

- **Архитектура и читаемость** кода, использование аннотаций, разбиение на модули — 25 баллов;
- **Корректность расчёта метрик** и наличие тестов — 25 баллов;
- **Производительность** (стриминг, работа с большими файлами) и устойчивость к «грязным» данным — 20 баллов;
- **Удобство CLI** и качество документации — 15 баллов;
- **Визуализации и экспорт** результатов — 10 баллов;
- **Бонусные задачи** (VoI, Docker, параллельность) — до +15 баллов.
