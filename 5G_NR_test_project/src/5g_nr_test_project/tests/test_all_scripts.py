# test_parsers_workflow.py
import sys
import os
from pathlib import Path
import csv
import json
from typing import List, Dict, Any
import random

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
current_dir = os.path.dirname( os.path.abspath( __file__ ) )
project_root = os.path.dirname( os.path.join( current_dir, '..', '..' ) )
sys.path.insert( 0, project_root )

from main_scripts.parsers import NJsonParser, CsvParser
from main_scripts.parser_factory import ParserFactory, get_parser_factory
from configs.models import ParseResult


class ParserTester:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç–µ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø–∞—Ä—Å–µ—Ä–æ–≤"""

    def __init__(self):
        self.test_dir = Path( "../main_scripts/test_results" )
        self.test_dir.mkdir( exist_ok=True )
        self.factory = get_parser_factory()
        self.record_count = 30  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–æ 30 –∑–∞–ø–∏—Å–µ–π

    def generate_test_data(self) -> tuple:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (30 –∑–∞–ø–∏—Å–µ–π)"""
        ndjson_data = []
        csv_data = []

        base_time = 0
        cars = [f"car_{i}" for i in range( 1, 11 )]  # 10 —Ä–∞–∑–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
        apps = ["BSM", "SPAT", "MAP", "RSI"]

        for i in range( self.record_count ):
            # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø–∏—Å–∏
            ts_us = base_time + i * 100
            src = random.choice( cars )
            dst = random.choice( [c for c in cars if c != src] )
            pkt_id = f"pkt_{i:03d}"
            app = random.choice( apps )
            bytes_size = random.randint( 200, 500 )

            # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º —Å–æ–±—ã—Ç–∏–µ (tx –∏–ª–∏ rx)
            if i % 3 == 0:  # –ö–∞–∂–¥–∞—è —Ç—Ä–µ—Ç—å—è –∑–∞–ø–∏—Å—å - rx, –æ—Å—Ç–∞–ª—å–Ω—ã–µ tx
                event = "rx"
                rssi = round( random.uniform( -80, -60 ), 1 )
                sinr = round( random.uniform( 10, 25 ), 1 )
            else:
                event = "tx"
                rssi = round( random.uniform( -70, -50 ), 1 )
                sinr = round( random.uniform( 15, 30 ), 1 )

            # NDJSON –∑–∞–ø–∏—Å—å
            ndjson_record = {
                "ts_us": ts_us,
                "event": event,
                "src": src,
                "dst": dst,
                "pkt_id": pkt_id,
                "app": app,
                "bytes": bytes_size,
                "rssi_dbm": rssi,
                "sinr_db": sinr
            }
            ndjson_data.append( ndjson_record )

            # CSV –∑–∞–ø–∏—Å—å (–≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏)
            csv_record = {
                "ts_us": str( ts_us ),
                "event": event,
                "src": src,
                "dst": dst,
                "pkt_id": pkt_id,
                "app": app,
                "bytes": str( bytes_size ),
                "rssi_dbm": str( rssi ) if random.random() > 0.2 else "",  # 20% –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                "sinr_db": str( sinr ) if random.random() > 0.2 else "",  # 20% –ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                "drop_reason": ""  # –í—Å–µ–≥–¥–∞ –ø—É—Å—Ç–æ–µ –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            }
            csv_data.append( csv_record )

        return ndjson_data, csv_data

    def create_test_files(self) -> Dict[str, Path]:
        """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Å 30 –∑–∞–ø–∏—Å—è–º–∏"""
        test_files = {}
        ndjson_data, csv_data = self.generate_test_data()

        # 1. –°–æ–∑–¥–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π NDJSON —Ñ–∞–π–ª
        ndjson_path = self.test_dir / "test_data_30.ndjson"
        with open( ndjson_path, 'w', encoding='utf-8' ) as f:
            for record in ndjson_data:
                f.write( json.dumps( record ) + '\n' )
        test_files['ndjson'] = ndjson_path

        # 2. –°–æ–∑–¥–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π CSV —Ñ–∞–π–ª
        csv_path = self.test_dir / "test_data_30.csv"
        with open( csv_path, 'w', newline='', encoding='utf-8' ) as f:
            if csv_data:
                writer = csv.DictWriter( f, fieldnames=csv_data[0].keys() )
                writer.writeheader()
                writer.writerows( csv_data )
        test_files['csv'] = csv_path

        # 3. –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –æ—à–∏–±–∫–∞–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
        error_ndjson_data = [
            '{"ts_us": 1000, "event": "tx", "src": "car_1", "dst": "car_2", "pkt_id": "a1", "app": "BSM", "bytes": 250}',
            'INVALID JSON LINE',
            '{"ts_us": "not_number", "event": "tx", "src": "car_1", "dst": "car_2", "pkt_id": "a2", "app": "BSM", "bytes": 250}',
            '{"ts_us": 1100, "event": "invalid_event", "src": "car_1", "dst": "car_2", "pkt_id": "a3", "app": "BSM", "bytes": 250}',
            '{"ts_us": 1200, "event": "tx", "src": "", "dst": "car_2", "pkt_id": "a4", "app": "BSM", "bytes": 250}',
            # –ü—É—Å—Ç–æ–π src
            '{"ts_us": 1300, "event": "rx", "src": "car_1", "dst": "", "pkt_id": "a5", "app": "BSM", "bytes": 250}',
            # –ü—É—Å—Ç–æ–π dst
        ]

        error_ndjson_path = self.test_dir / "test_errors.ndjson"
        with open( error_ndjson_path, 'w', encoding='utf-8' ) as f:
            for line in error_ndjson_data:
                f.write( line + '\n' )
        test_files['ndjson_errors'] = error_ndjson_path

        print( "‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:" )
        for name, path in test_files.items():
            file_size = path.stat().st_size
            print( f"   - {name}: {path} ({file_size} bytes)" )

        return test_files

    def test_parser_factory(self, test_files: Dict[str, Path]):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ñ–∞–±—Ä–∏–∫—É –ø–∞—Ä—Å–µ—Ä–æ–≤"""
        print( "\nüîß –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –§–ê–ë–†–ò–ö–ò –ü–ê–†–°–ï–†–û–í" )
        print( "-" * 50 )

        for format_name, file_path in test_files.items():
            if 'error' not in format_name:
                try:
                    parser = self.factory.get_parser( file_path )
                    print( f"‚úÖ {format_name.upper()}: {parser.__class__.__name__}" )
                    print( f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: {parser.supported_extensions}" )
                except Exception as e:
                    print( f"‚ùå {format_name.upper()}: –û—à–∏–±–∫–∞ - {e}" )

    def test_njson_parser(self, test_files: Dict[str, Path]):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç NDJSON –ø–∞—Ä—Å–µ—Ä"""
        print( "\nüìÑ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï NDJSON –ü–ê–†–°–ï–†–ê" )
        print( "-" * 50 )

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–∞–π–ª
        parser = NJsonParser()
        valid_file = test_files['ndjson']

        print( f"–¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª: {valid_file}" )

        try:
            records = list( parser.parse_data_stream( valid_file ) )
            stats = parser.get_stats()

            print( f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—à–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len( records )} –∏–∑ {self.record_count}" )
            print( f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}" )

            if records:
                print( "\n–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø–∏—Å–µ–π:" )
                for i in range( min( 2, len( records ) ) ):
                    record = records[i]
                    print( f"  –ó–∞–ø–∏—Å—å {i + 1}:" )
                    print( f"    ts_us: {record.ts_us}, event: {record.event}" )
                    print( f"    src: {record.src} -> dst: {record.dst}" )
                    print( f"    pkt_id: {record.pkt_id}, app: {record.app}" )
                    print( f"    bytes: {record.bytes}" )

        except Exception as e:
            print( f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}" )

    def test_csv_parser(self, test_files: Dict[str, Path]):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç CSV –ø–∞—Ä—Å–µ—Ä"""
        print( "\nüìä –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï CSV –ü–ê–†–°–ï–†–ê" )
        print( "-" * 50 )

        parser = CsvParser()
        csv_file = test_files['csv']

        print( f"–¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª: {csv_file}" )

        try:
            records = list( parser.parse_data_stream( csv_file ) )
            stats = parser.get_stats()

            print( f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—à–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len( records )} –∏–∑ {self.record_count}" )
            print( f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}" )

            if records:
                print( "\n–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø–∏—Å–µ–π:" )
                for i in range( min( 2, len( records ) ) ):
                    record = records[i]
                    print( f"  –ó–∞–ø–∏—Å—å {i + 1}:" )
                    print( f"    ts_us: {record.ts_us}, event: {record.event}" )
                    print( f"    src: {record.src} -> dst: {record.dst}" )
                    print( f"    pkt_id: {record.pkt_id}, app: {record.app}" )
                    print( f"    bytes: {record.bytes}" )
                    if record.rssi_dbm:
                        print( f"    rssi_dbm: {record.rssi_dbm}" )

        except Exception as e:
            print( f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}" )

    def test_streaming_capability(self, test_files: Dict[str, Path]):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç streaming –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–∞—Ä—Å–µ—Ä–æ–≤"""
        print( "\n‚ö° –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï STREAMING –†–ï–ñ–ò–ú–ê" )
        print( "-" * 50 )

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º NDJSON streaming
        print( "NDJSON Streaming —Ç–µ—Å—Ç:" )
        parser = NJsonParser()
        records_yielded = 0
        start_memory = self._get_memory_usage()

        for record in parser.parse_data_stream( test_files['ndjson'] ):
            records_yielded += 1
            if records_yielded <= 3:
                print( f"  Yielded record {records_yielded}: {record.pkt_id}" )

        end_memory = self._get_memory_usage()
        print( f"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π —á–µ—Ä–µ–∑ streaming: {records_yielded}" )
        print( f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {end_memory - start_memory:.2f} MB" )

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º CSV streaming
        print( "\nCSV Streaming —Ç–µ—Å—Ç:" )
        csv_parser = CsvParser()
        records_yielded = 0
        start_memory = self._get_memory_usage()

        for record in csv_parser.parse_data_stream( test_files['csv'] ):
            records_yielded += 1
            if records_yielded <= 3:
                print( f"  Yielded record {records_yielded}: {record.pkt_id}" )

        end_memory = self._get_memory_usage()
        print( f"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π —á–µ—Ä–µ–∑ streaming: {records_yielded}" )
        print( f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {end_memory - start_memory:.2f} MB" )

    def _get_memory_usage(self) -> float:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ MB"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0.0

    def test_validation(self, test_files: Dict[str, Path]):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        print( "\nüîç –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –í–ê–õ–ò–î–ê–¶–ò–ò –§–ê–ô–õ–û–í" )
        print( "-" * 50 )

        parsers = [NJsonParser(), CsvParser()]

        for parser in parsers:
            print( f"\n{parser.__class__.__name__}:" )
            for format_name, file_path in test_files.items():
                is_valid = parser.validate_file_format( file_path )
                status = "‚úÖ –í–ê–õ–ò–î–ï–ù" if is_valid else "‚ùå –ù–ï–í–ê–õ–ò–î–ï–ù"
                print( f"  {file_path.name}: {status}" )

    def test_performance(self, test_files: Dict[str, Path]):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—Ä—Å–µ—Ä–æ–≤"""
        print( "\n‚è±Ô∏è  –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò" )
        print( "-" * 50 )

        import time

        # NDJSON performance
        parser = NJsonParser()
        start_time = time.time()

        records = list( parser.parse_data_stream( test_files['ndjson'] ) )
        ndjson_time = time.time() - start_time

        print( f"NDJSON –ø–∞—Ä—Å–µ—Ä: {ndjson_time:.4f} —Å–µ–∫—É–Ω–¥ –¥–ª—è {len( records )} –∑–∞–ø–∏—Å–µ–π" )
        print( f"–°–∫–æ—Ä–æ—Å—Ç—å: {len( records ) / ndjson_time:.2f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫—É–Ω–¥—É" )

        # CSV performance
        csv_parser = CsvParser()
        start_time = time.time()

        records = list( csv_parser.parse_data_stream( test_files['csv'] ) )
        csv_time = time.time() - start_time

        print( f"CSV –ø–∞—Ä—Å–µ—Ä: {csv_time:.4f} —Å–µ–∫—É–Ω–¥ –¥–ª—è {len( records )} –∑–∞–ø–∏—Å–µ–π" )
        print( f"–°–∫–æ—Ä–æ—Å—Ç—å: {len( records ) / csv_time:.2f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫—É–Ω–¥—É" )

    def test_edge_cases(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫—Ä–∞–π–Ω–∏–µ —Å–ª—É—á–∞–∏"""
        print( "\nüö© –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–†–ê–ô–ù–ò–• –°–õ–£–ß–ê–ï–í" )
        print( "-" * 50 )

        # –¢–µ—Å—Ç 1: –ù–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª
        print( "1. –¢–µ—Å—Ç –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞:" )
        try:
            parser = self.factory.get_parser( Path( "nonexistent_file.xyz" ) )
            print( "‚ùå –û–∂–∏–¥–∞–ª–∞—Å—å –æ—à–∏–±–∫–∞ –¥–ª—è –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞" )
        except (ValueError, FileNotFoundError) as e:
            print( f"‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –æ—à–∏–±–∫–∞: {e}" )

        # –¢–µ—Å—Ç 2: –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        print( "\n2. –¢–µ—Å—Ç –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞:" )
        unsupported_file = self.test_dir / "test.unsupported"
        unsupported_file.write_text( "some data" )
        try:
            parser = self.factory.get_parser( unsupported_file )
            print( "‚ùå –û–∂–∏–¥–∞–ª–∞—Å—å –æ—à–∏–±–∫–∞ –¥–ª—è –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞" )
        except ValueError as e:
            print( f"‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –æ—à–∏–±–∫–∞: {e}" )
        finally:
            if unsupported_file.exists():
                unsupported_file.unlink()

    def run_all_tests(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ —Ç–µ—Å—Ç—ã"""
        print( "üöÄ –ó–ê–ü–£–°–ö –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ü–ê–†–°–ï–†–û–í" )
        print( "=" * 60 )
        print( f"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å {self.record_count} –∑–∞–ø–∏—Å—è–º–∏ –Ω–∞ —Ñ–∞–π–ª" )

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        test_files = self.create_test_files()

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
        self.test_parser_factory( test_files )
        self.test_njson_parser( test_files )
        self.test_csv_parser( test_files )
        self.test_streaming_capability( test_files )
        self.test_validation( test_files )
        self.test_performance( test_files )
        self.test_edge_cases()

        print( "\n" + "=" * 60 )
        print( "üéâ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!" )

        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        #self.cleanup_test_files( test_files )

    def cleanup_test_files(self, test_files: Dict[str, Path]):
        """–û—á–∏—â–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã"""
        print( "\nüßπ –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤..." )
        for file_path in test_files.values():
            if file_path.exists():
                file_path.unlink()
                print( f"   –£–¥–∞–ª–µ–Ω: {file_path}" )

        # –£–¥–∞–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –ø—É—Å—Ç–∞—è
        if self.test_dir.exists() and not any( self.test_dir.iterdir() ):
            self.test_dir.rmdir()
            print( f"   –£–¥–∞–ª–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.test_dir}" )


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    try:
        tester = ParserTester()
        tester.run_all_tests()
    except Exception as e:
        print( f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}" )
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit( main() )
